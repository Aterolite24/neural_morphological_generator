# -*- coding: utf-8 -*-
"""nmg2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/101H7ztDOnjvVYJ6_dPXVLQpwoNxNoOXm
"""

import numpy as np
import pandas as pd
import os
from google.colab import drive
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras import Model, Input
from tensorflow.keras.layers import LSTM, Embedding, Dense, Attention, MultiHeadAttention, LayerNormalization, Dropout, Add, GRU, Bidirectional, RepeatVector, Flatten, Concatenate
from tensorflow.keras.preprocessing.sequence import pad_sequences

drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/nmg/hin/processed_dataset_hin.csv')
df.head()

base_words = df['Base Word'].unique().tolist()
transformed_words = df['Transformed Word'].unique().tolist()
all_words = base_words + transformed_words

tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_words)

vocab_size = len(tokenizer.word_index) + 1
max_len = max(df['Base Word'].apply(lambda x: len(x.split())))

x_text = tokenizer.texts_to_sequences(df['Base Word'])
y_text = tokenizer.texts_to_sequences(df['Transformed Word'])
x_text = pad_sequences(x_text, maxlen=max_len, padding='post')
y_text = pad_sequences(y_text, maxlen=max_len, padding='post')

encoder = OneHotEncoder(sparse_output=False)
features = df[['POS', 'Person', 'Number', 'Tense', 'Aspect', 'Gender']].values
features_onehot = encoder.fit_transform(features)

x_train, x_val, y_train, y_val, features_train, features_val = train_test_split(
    x_text, y_text, features_onehot, test_size=0.2, random_state=42
)

from tensorflow.keras.layers import Layer, Input, Embedding, LSTM, Dense, Concatenate, Bidirectional, TimeDistributed, RepeatVector
from tensorflow.keras.models import Model
import tensorflow as tf

class AttentionLayer(Layer):
    def call(self, inputs):
        decoder_outputs, encoder_outputs = inputs
        attention_scores = tf.matmul(decoder_outputs, encoder_outputs, transpose_b=True)
        attention_weights = tf.nn.softmax(attention_scores, axis=-1)
        context_vector = tf.matmul(attention_weights, encoder_outputs)
        return context_vector


def build_seq2seq_attention(vocab_size, embedding_dim, lstm_units, max_len, feature_dim):
    # Encoder
    encoder_inputs = Input(shape=(max_len,))
    encoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)
    encoder_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True))
    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)
    encoder_state_h = Concatenate()([forward_h, backward_h])
    encoder_state_c = Concatenate()([forward_c, backward_c])

    # Decoder
    decoder_inputs = Input(shape=(max_len,))
    decoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)
    decoder_lstm = LSTM(lstm_units * 2, return_sequences=True, return_state=False)
    decoder_outputs = decoder_lstm(decoder_embedding, initial_state=[encoder_state_h, encoder_state_c])

    # Attention Layer
    context_vector = AttentionLayer()([decoder_outputs, encoder_outputs])

    # Feature Input
    feature_inputs = Input(shape=(feature_dim,))
    feature_dense = Dense(128, activation="relu")(feature_inputs)
    expanded_features = RepeatVector(max_len)(feature_dense)

    # Combine Attention Context and Features
    combined = Concatenate(axis=-1)([context_vector, expanded_features])

    # Output Layer
    output_layer = TimeDistributed(Dense(vocab_size, activation="softmax"))(combined)

    # Model
    model = Model(inputs=[encoder_inputs, decoder_inputs, feature_inputs], outputs=output_layer)
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    return model

embedding_dim = 100
lstm_units = 512
feature_dim = features_train.shape[1]

model = build_seq2seq_attention(vocab_size, embedding_dim, lstm_units, max_len, feature_dim)
model.summary()

history = model.fit(
    [x_train, x_train, features_train],
    np.expand_dims(y_train, -1),
    batch_size=64,
    epochs=50,
    validation_data=([x_val, x_val, features_val], np.expand_dims(y_val, -1))
)

val_loss, val_accuracy = model.evaluate(
    [x_val, x_val, features_val], np.expand_dims(y_val, -1)
)
print(f"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}")

from nltk.translate.bleu_score import sentence_bleu

def calculate_bleu_score(predictions, actuals):
    scores = [sentence_bleu([reference], prediction) for prediction, reference in zip(predictions, actuals)]
    return np.mean(scores)

val_predictions = model.predict([x_val, x_val, features_val])
val_predictions = np.argmax(val_predictions, axis=-1)  # Convert to token indices
predictions_text = [" ".join([tokenizer.index_word[idx] for idx in seq if idx > 0]) for seq in val_predictions]
actuals_text = [" ".join([tokenizer.index_word[idx] for idx in seq if idx > 0]) for seq in y_val]
bleu_score = calculate_bleu_score(predictions_text, actuals_text)
print(f"BLEU Score: {bleu_score}")

import matplotlib.pyplot as plt

plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.legend()
plt.title('Training Metrics')
plt.show()

model_name = "Seq2Seq"
evaluation_results = {model_name: {'bleu_score': bleu_score}}

bleu_scores = [eval['bleu_score'] for eval in evaluation_results.values()]
plt.bar(evaluation_results.keys(), bleu_scores, color='skyblue')
plt.ylabel('BLEU Score')
plt.title('Model Comparison - BLEU Score')
plt.show()

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

y_pred = np.argmax(val_predictions, axis=-1).flatten()
y_true = np.argmax(y_val, axis=-1).flatten()
mask = y_true != 0
y_pred_filtered = y_pred[mask]
y_true_filtered = y_true[mask]

cm = confusion_matrix(y_true_filtered, y_pred_filtered)

accuracy = accuracy_score(y_true_filtered, y_pred_filtered)
precision = precision_score(y_true_filtered, y_pred_filtered, average='weighted', zero_division=0)
recall = recall_score(y_true_filtered, y_pred_filtered, average='weighted', zero_division=0)
f1 = f1_score(y_true_filtered, y_pred_filtered, average='weighted', zero_division=0)


print("Confusion Matrix:\n", cm)
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix")
plt.show()